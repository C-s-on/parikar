{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9d87d336-020e-4a8d-a63c-f67be695151f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8ad6e65b-264f-49c4-a61d-5de622431c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Desired input shape for VGG16\n",
    "input_shape = (224, 224, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4902478a-2355-4707-9efe-bbb2a32c4b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ImageDataGenerator for rescaling and resizing and data_augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1./255,  # Rescale pixel values to the range [0, 1]\n",
    "    validation_split=0.2,  # Optional: Split data into training and validation sets\n",
    "    preprocessing_function=lambda x: tf.image.resize(x, (input_shape[0], input_shape[1])),\n",
    "    rotation_range=20,  # Rotate images by a random angle within the specified range\n",
    "    width_shift_range=0.2,  # Shift images horizontally by a fraction of total width\n",
    "    height_shift_range=0.2,  # Shift images vertically by a fraction of total height\n",
    "    shear_range=0.2,  # Apply shear transformation\n",
    "    zoom_range=0.2,  # Zoom in/out on images\n",
    "    horizontal_flip=True,  # Randomly flip images horizontally\n",
    "    vertical_flip=True,\n",
    "    brightness_range=[0.5, 1.5],  # Adjust brightness\n",
    "#   contrast_range=[0.5, 1.5],  # Adjust contrast\n",
    "    channel_shift_range=50,  # Shift channel values\n",
    "    fill_mode='nearest'  # Fill in newly created pixels after rotation or shifting\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "07b4d592-9490-4baf-96fc-b554873b1cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to your dataset\n",
    "dataset_path = \"C:/Users/user_99/Desktop/data_of_parikar\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ffbf7dbc-8d90-43a9-9548-dcdf7353c308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10239 images belonging to 12 classes.\n",
      "Found 2555 images belonging to 12 classes.\n"
     ]
    }
   ],
   "source": [
    "# Data generators for training and validation\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    dataset_path,\n",
    "    target_size=(input_shape[0], input_shape[1]),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='training'  # For training data\n",
    ")\n",
    "\n",
    "validation_generator = datagen.flow_from_directory(\n",
    "    dataset_path,\n",
    "    target_size=(input_shape[0], input_shape[1]),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'  # For validation data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5f1a70b6-d6c8-40bb-abd2-a176f72978a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-trained VGG16 model without the top (fully connected) layers\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b0aa70-c129-4426-87e1-c440ac2fcefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze the convolutional base, unfreeze the last 3 convolutional layers\n",
    "#Fine_tunning\n",
    "for layer in base_model.layers[:-3]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a6e1491d-bcb3-42af-a9ae-4b076ecaa655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new model\n",
    "model = Sequential()\n",
    "model.add(base_model)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(12, activation='softmax')) # Final Dense layer with 12 nodes (number of classes) and softmax activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7fc5c768-88c3-49a7-ab6b-91aebaafebc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer=optimizers.RMSprop(learning_rate=1e-4), \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2ab3c85c-3e14-4005-bc10-52d47d0ae565",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " vgg16 (Functional)          (None, 7, 7, 512)         14714688  \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 25088)             0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 256)               6422784   \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 12)                3084      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21140556 (80.64 MB)\n",
      "Trainable params: 11145484 (42.52 MB)\n",
      "Non-trainable params: 9995072 (38.13 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4e185c30-d4eb-45fb-8621-321f5dbe1fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "320/320 [==============================] - 624s 2s/step - loss: 1.1758 - accuracy: 0.6172 - val_loss: 0.8536 - val_accuracy: 0.7194\n",
      "Epoch 2/10\n",
      "320/320 [==============================] - 609s 2s/step - loss: 0.7583 - accuracy: 0.7593 - val_loss: 0.8326 - val_accuracy: 0.7256\n",
      "Epoch 3/10\n",
      "320/320 [==============================] - 611s 2s/step - loss: 0.6418 - accuracy: 0.7965 - val_loss: 0.5894 - val_accuracy: 0.8078\n",
      "Epoch 4/10\n",
      "320/320 [==============================] - 611s 2s/step - loss: 0.5834 - accuracy: 0.8121 - val_loss: 0.5363 - val_accuracy: 0.8278\n",
      "Epoch 5/10\n",
      "320/320 [==============================] - 610s 2s/step - loss: 0.5325 - accuracy: 0.8328 - val_loss: 0.5951 - val_accuracy: 0.8200\n",
      "Epoch 6/10\n",
      "320/320 [==============================] - 610s 2s/step - loss: 0.5153 - accuracy: 0.8405 - val_loss: 0.5982 - val_accuracy: 0.8215\n",
      "Epoch 7/10\n",
      "320/320 [==============================] - 611s 2s/step - loss: 0.4962 - accuracy: 0.8463 - val_loss: 0.6574 - val_accuracy: 0.8121\n",
      "Epoch 8/10\n",
      "320/320 [==============================] - 610s 2s/step - loss: 353.0864 - accuracy: 0.8632 - val_loss: 0.5466 - val_accuracy: 0.8477\n",
      "Epoch 9/10\n",
      "320/320 [==============================] - 611s 2s/step - loss: 0.4293 - accuracy: 0.8648 - val_loss: 0.4876 - val_accuracy: 0.8532\n",
      "Epoch 10/10\n",
      "320/320 [==============================] - 614s 2s/step - loss: 0.4375 - accuracy: 0.8682 - val_loss: 0.5180 - val_accuracy: 0.8470\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1a6e11c19a0>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model train\n",
    "model.fit(train_generator, epochs=10, validation_data=validation_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bf49ed72-9473-41eb-bfac-6f96849d9d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\user_99\\AppData\\Local\\Temp\\tmp5bgek5ud\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\user_99\\AppData\\Local\\Temp\\tmp5bgek5ud\\assets\n"
     ]
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0cd51d44-9513-43fe-8f85-5f0e59be41ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "tflite_model_path = \"C:/Users/user_99/Desktop/tflite_model.tflite\"\n",
    "with open(tflite_model_path, 'wb') as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52641d65-2672-493a-b276-21f20b4371d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
