{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d87d336-020e-4a8d-a63c-f67be695151f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import InceptionV3\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, BatchNormalization\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ad6e65b-264f-49c4-a61d-5de622431c7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Desired input shape for InceptionV3\n",
    "input_shape = (299, 299, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4902478a-2355-4707-9efe-bbb2a32c4b94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ImageDataGenerator for rescaling and resizing and data_augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1./255,  # Rescale pixel values to the range [0, 1]\n",
    "    validation_split=0.2,  # Optional: Split data into training and validation sets\n",
    "    preprocessing_function=lambda x: tf.image.resize(x, (input_shape[0], input_shape[1])),\n",
    "    rotation_range=20,  # Rotate images by a random angle within the specified range\n",
    "    width_shift_range=0.2,  # Shift images horizontally by a fraction of total width\n",
    "    height_shift_range=0.2,  # Shift images vertically by a fraction of total height\n",
    "    shear_range=0.2,  # Apply shear transformation\n",
    "    zoom_range=0.2,  # Zoom in/out on images\n",
    "    horizontal_flip=True,  # Randomly flip images horizontally\n",
    "    vertical_flip=True,\n",
    "    brightness_range=[0.5, 1.5],  # Adjust brightness\n",
    "#   contrast_range=[0.5, 1.5],  # Adjust contrast\n",
    "    channel_shift_range=50,  # Shift channel values\n",
    "    fill_mode='nearest'  # Fill in newly created pixels after rotation or shifting\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07b4d592-9490-4baf-96fc-b554873b1cca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Path to your dataset\n",
    "dataset_path = \"C:/Users/user_99/Desktop/data_of_parikar\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffbf7dbc-8d90-43a9-9548-dcdf7353c308",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10239 images belonging to 12 classes.\n",
      "Found 2555 images belonging to 12 classes.\n"
     ]
    }
   ],
   "source": [
    "# Data generators for training and validation\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    dataset_path,\n",
    "    target_size=(input_shape[0], input_shape[1]),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='training'  # For training data\n",
    ")\n",
    "\n",
    "validation_generator = datagen.flow_from_directory(\n",
    "    dataset_path,\n",
    "    target_size=(input_shape[0], input_shape[1]),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'  # For validation data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f1a70b6-d6c8-40bb-abd2-a176f72978a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Pre-trained InceptionV3 model without the top (fully connected) layers\n",
    "base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40b0aa70-c129-4426-87e1-c440ac2fcefc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Freeze the convolutional base, unfreeze the last 3 convolutional layers\n",
    "#Fine_tunning\n",
    "for layer in base_model.layers[:-50]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6e1491d-bcb3-42af-a9ae-4b076ecaa655",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a new model\n",
    "model = Sequential()\n",
    "model.add(base_model)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(12, activation='softmax')) # Final Dense layer with 12 nodes (number of classes) and softmax activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7fc5c768-88c3-49a7-ab6b-91aebaafebc6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer=optimizers.RMSprop(learning_rate=1e-4), \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ab3c85c-3e14-4005-bc10-52d47d0ae565",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " vgg16 (Functional)          (None, 7, 7, 512)         14714688  \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 25088)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               6422784   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 12)                3084      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21,140,556\n",
      "Trainable params: 11,145,484\n",
      "Non-trainable params: 9,995,072\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2f32d7-aba1-4efc-90b5-d8c03b08c2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ModelCheckpoint callback to save the weights\n",
    "checkpoint_filepath = \"C:/Users/user_99/Desktop/weights_checkpoint.h5\"\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    save_best_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e185c30-d4eb-45fb-8621-321f5dbe1fee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " 17/320 [>.............................] - ETA: 2:19 - loss: 2.3867 - accuracy: 0.2077"
     ]
    }
   ],
   "source": [
    "# Train the model using the data generators with the ModelCheckpoint callback\n",
    "model.fit(train_generator, epochs=100, validation_data=validation_generator, callbacks=[model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33006ebd-95fe-49df-9e3f-d313010b789a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the epoch from which you want to convert weights to TensorFlow Lite\n",
    "#desired_epoch = 50  # Change this to the desired epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea02a458-12f7-42ef-82b4-9321d341a9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the model to TensorFlow Lite with post-training quantization using the saved weights\n",
    "# model.load_weights(checkpoint_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bf49ed72-9473-41eb-bfac-6f96849d9d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\user_99\\AppData\\Local\\Temp\\tmp5bgek5ud\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\user_99\\AppData\\Local\\Temp\\tmp5bgek5ud\\assets\n"
     ]
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "tflite_model_path = \"C:/Users/user_99/Desktop/tflite_model.tflite\"\n",
    "with open(tflite_model_path, 'wb') as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9598830b-479a-4cf8-aab9-ba667131588f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14216\\1005725661.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Convert the model to TensorFlow Lite with post-training quantization\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mconverter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTFLiteConverter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_keras_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mconverter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOptimize\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDEFAULT\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtflite_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconverter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# # Convert the model to TensorFlow Lite with post-training quantization\n",
    "# converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "# converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "# tflite_model = converter.convert()\n",
    "\n",
    "# tflite_model_path = \"C:/Users/user_99/Desktop/tflite_model.tflite\"\n",
    "# with open(tflite_model_path, 'wb') as f:\n",
    "#     f.write(tflite_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
